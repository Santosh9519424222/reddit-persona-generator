{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97979ac3-20a8-473c-9097-5eebcd62d11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Reddit Profile URL:  https://www.reddit.com/user/kojied/comments/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Extracted username: kojied\n",
      "âœ… Persona saved as TXT: kojied_persona.txt\n",
      "âœ… Persona saved as PDF: kojied_persona.pdf\n",
      "âœ… Persona saved as JSON: kojied_persona.json\n",
      "ðŸŽ‰ All outputs generated successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from fpdf import FPDF\n",
    "import openai\n",
    "import praw\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Reddit API Credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id='RIG4mlm3BHo2AT-0bLOEYg',\n",
    "    client_secret='ufS3x-JSj9HqBRXbE63CBvu9KMpk1Q',\n",
    "    user_agent='Reddit Persona Script by Santosh'\n",
    ")\n",
    "# OpenAI API setup\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Extract username from URL\n",
    "def extract_username(profile_url):\n",
    "    parts = profile_url.rstrip('/').split('/')\n",
    "    if 'user' in parts:\n",
    "        return parts[parts.index('user') + 1]\n",
    "    return None\n",
    "\n",
    "# Scrape posts & comments\n",
    "def scrape_user_data(username, comment_limit=10, post_limit=5):\n",
    "    user = reddit.redditor(username)\n",
    "    data = {'comments': [], 'posts': []}\n",
    "\n",
    "    for comment in user.comments.new(limit=comment_limit):\n",
    "        data['comments'].append({\n",
    "            'text': comment.body,\n",
    "            'subreddit': str(comment.subreddit),\n",
    "            'url': f\"https://www.reddit.com{comment.permalink}\"\n",
    "        })\n",
    "\n",
    "    for submission in user.submissions.new(limit=post_limit):\n",
    "        data['posts'].append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'subreddit': str(submission.subreddit),\n",
    "            'url': f\"https://www.reddit.com{submission.permalink}\"\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Combine text\n",
    "def combine_text(data):\n",
    "    combined_text = \"=== POSTS ===\\n\\n\"\n",
    "    for post in data['posts']:\n",
    "        combined_text += f\"Title: {post['title']}\\nContent: {post['body']}\\nSubreddit: {post['subreddit']}\\nURL: {post['url']}\\n\\n\"\n",
    "\n",
    "    combined_text += \"=== COMMENTS ===\\n\\n\"\n",
    "    for comment in data['comments']:\n",
    "        combined_text += f\"Comment: {comment['text']}\\nSubreddit: {comment['subreddit']}\\nURL: {comment['url']}\\n\\n\"\n",
    "\n",
    "    return combined_text\n",
    "\n",
    "# Generate persona with OpenAI\n",
    "def generate_persona_with_citations(combined_text):\n",
    "    prompt = f\"\"\"\n",
    "You are an AI that builds a User Persona based on Reddit posts and comments.\n",
    "\n",
    "Instructions:\n",
    "- Identify Personality Traits\n",
    "- Identify Interests\n",
    "- Describe Behavior & Habits\n",
    "- List Motivations\n",
    "- Identify Frustrations\n",
    "- Describe Goals & Needs\n",
    "- Provide example quotes from posts/comments for each insight\n",
    "- Mention subreddit names wherever possible\n",
    "\n",
    "Here is the Reddit user's content:\n",
    "\n",
    "{combined_text}\n",
    "\n",
    "Return the analysis as a well-structured User Persona.\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Save as TXT\n",
    "def save_as_txt(username, persona_text):\n",
    "    filename = f\"{username}_persona.txt\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(persona_text)\n",
    "    print(f\"âœ… Persona saved as TXT: {filename}\")\n",
    "\n",
    "from fpdf import FPDF\n",
    "\n",
    "def save_as_pdf(username, persona_text):\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    \n",
    "    # Set font that supports UTF-8\n",
    "    pdf.add_font('DejaVu', '', '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', uni=True)\n",
    "    pdf.set_font('DejaVu', '', 12)\n",
    "    \n",
    "    # Write text line by line\n",
    "    for line in persona_text.split('\\n'):\n",
    "        pdf.multi_cell(0, 10, line)\n",
    "    \n",
    "    filename = f\"{username}_persona.pdf\"\n",
    "    pdf.output(filename)\n",
    "    print(f\"âœ… Persona saved as PDF: {filename}\")\n",
    "\n",
    "# Save as JSON\n",
    "def save_as_json(username, persona_text, scraped_data):\n",
    "    data = {\n",
    "        'username': username,\n",
    "        'persona': persona_text,\n",
    "        'scraped_data': scraped_data\n",
    "    }\n",
    "    filename = f\"{username}_persona.json\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"âœ… Persona saved as JSON: {filename}\")\n",
    "\n",
    "# ======= MAIN EXECUTION =======\n",
    "if __name__ == \"__main__\":\n",
    "    reddit_url = input(\"Enter Reddit Profile URL: \")\n",
    "    username = extract_username(reddit_url)\n",
    "    print(f\"ðŸ“¥ Extracted username: {username}\")\n",
    "\n",
    "    scraped_data = scrape_user_data(username, comment_limit=10, post_limit=5)\n",
    "    combined_text = combine_text(scraped_data)\n",
    "    persona_output = generate_persona_with_citations(combined_text)\n",
    "\n",
    "    # Save outputs\n",
    "    save_as_txt(username, persona_output)      # TXT output\n",
    "    save_as_pdf(username, persona_output)      # PDF output\n",
    "    save_as_json(username, persona_output, scraped_data)  # JSON output\n",
    "\n",
    "    print(\"ðŸŽ‰ All outputs generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f56f20a-6c47-4b13-ba0c-85466954d90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git version 2.25.1\n"
     ]
    }
   ],
   "source": [
    "!git --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cfd81-363f-468d-9d56-f7ba62ea00d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201c606-698b-4dc3-88fc-ff5262a7f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git remote add origin https://github.com/Santosh9519424222/reddit-persona-generator.git\n",
    "!git add .\n",
    "!git commit -m \"Initial commit - Reddit User Persona Project\"\n",
    "!git branch -M main\n",
    "!git push -u origin main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bee159-a3b9-4bef-9a94-382a21d3fc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
